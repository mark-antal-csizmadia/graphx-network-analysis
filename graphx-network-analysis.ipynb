{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sources:\n",
    "\n",
    "https://graphframes.github.io/graphframes/docs/_site/user-guide.html\n",
    "\n",
    "https://docs.databricks.com/_static/notebooks/graphframes-user-guide-py.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://graphframes.github.io/graphframes/docs/_site/quick-start.html\n",
    "# https://stackoverflow.com/questions/65011599/how-to-start-graphframes-on-spark-on-pyspark-on-juypter-on-docker\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages graphframes:graphframes:0.8.1-spark3.0-s_2.12 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/19 21:48:49 WARN Utils: Your hostname, mark-machine resolves to a loopback address: 127.0.1.1; using 192.168.0.102 instead (on interface wlp8s0)\n",
      "21/10/19 21:48:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/mark/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mark/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4e37376d-3dd9-4549-b252-c9d8ce911acb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.1-spark3.0-s_2.12 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 109ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.1-spark3.0-s_2.12 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4e37376d-3dd9-4549-b252-c9d8ce911acb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/4ms)\n",
      "21/10/19 21:48:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Start Spark context\n",
    "spark = SparkSession.builder.appName('graphx-network-analysis').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-19 21:48:52--  http://snap.stanford.edu/data/amazon0302.txt.gz\n",
      "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
      "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4664334 (4,4M) [application/x-gzip]\n",
      "Saving to: ‘data/amazon0302.txt.gz.1’\n",
      "\n",
      "amazon0302.txt.gz.1 100%[===================>]   4,45M   417KB/s    in 11s     \n",
      "\n",
      "2021-10-19 21:49:04 (401 KB/s) - ‘data/amazon0302.txt.gz.1’ saved [4664334/4664334]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p data\n",
    "!wget http://snap.stanford.edu/data/amazon0302.txt.gz -P data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/amazon0302.txt.gz:\t 73.0% -- created data/amazon0302.txt\n"
     ]
    }
   ],
   "source": [
    "!gzip -dkfv data/amazon0302.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(from_file_name, exclude_meta=True):\n",
    "    df_edges = pd.read_csv(from_file_name, sep=\"\\t\", header=None, skiprows=4, names=[\"src\", \"dst\"])\n",
    "    vertices_np = \\\n",
    "        np.unique(np.hstack([df_edges[\"src\"].unique(), df_edges[\"dst\"].unique()]))\n",
    "    \n",
    "    df_vertices = pd.DataFrame(data={\"id\": vertices_np})\n",
    "    \n",
    "    meta_cols = [\"ansi\", \"title\", \"group\", \"salesrank\", \"similar\", \"categories\", \"reviews\"]\n",
    "    \n",
    "    if exclude_meta:\n",
    "        df_vertices[meta_cols] = np.nan\n",
    "    else:\n",
    "        # read meta file and construct column values\n",
    "        # until then same as no meta\n",
    "        df_vertices[meta_cols] = np.nan\n",
    "\n",
    "    \n",
    "    return df_edges, df_vertices\n",
    "    \n",
    "from_file_name = \"data/amazon0302.txt\"\n",
    "df_edges, df_vertices = read_data(from_file_name=from_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            src     dst\n",
      "0             0       1\n",
      "1             0       2\n",
      "2             0       3\n",
      "3             0       4\n",
      "4             0       5\n",
      "...         ...     ...\n",
      "1234872  262110  262099\n",
      "1234873  262110  262100\n",
      "1234874  262110  262101\n",
      "1234875  262110  262106\n",
      "1234876  262110  262107\n",
      "\n",
      "[1234877 rows x 2 columns]\n",
      "            id  ansi  title  group  salesrank  similar  categories  reviews\n",
      "0            0   NaN    NaN    NaN        NaN      NaN         NaN      NaN\n",
      "1            1   NaN    NaN    NaN        NaN      NaN         NaN      NaN\n",
      "2            2   NaN    NaN    NaN        NaN      NaN         NaN      NaN\n",
      "3            3   NaN    NaN    NaN        NaN      NaN         NaN      NaN\n",
      "4            4   NaN    NaN    NaN        NaN      NaN         NaN      NaN\n",
      "...        ...   ...    ...    ...        ...      ...         ...      ...\n",
      "262106  262106   NaN    NaN    NaN        NaN      NaN         NaN      NaN\n",
      "262107  262107   NaN    NaN    NaN        NaN      NaN         NaN      NaN\n",
      "262108  262108   NaN    NaN    NaN        NaN      NaN         NaN      NaN\n",
      "262109  262109   NaN    NaN    NaN        NaN      NaN         NaN      NaN\n",
      "262110  262110   NaN    NaN    NaN        NaN      NaN         NaN      NaN\n",
      "\n",
      "[262111 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_edges)\n",
    "print(df_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = spark.createDataFrame(df_vertices, list(df_vertices.columns.values))\n",
    "edges = spark.createDataFrame(df_edges, list(df_edges.columns.values))\n",
    "g = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+-----+---------+-------+----------+-------+\n",
      "| id|ansi|title|group|salesrank|similar|categories|reviews|\n",
      "+---+----+-----+-----+---------+-------+----------+-------+\n",
      "|  0| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "|  1| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "|  2| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "|  3| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "|  4| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "|  5| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "|  6| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "|  7| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "|  8| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "|  9| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "| 10| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "| 11| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "| 12| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "| 13| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "| 14| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "| 15| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "| 16| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "| 17| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "| 18| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "| 19| NaN|  NaN|  NaN|      NaN|    NaN|       NaN|    NaN|\n",
      "+---+----+-----+-----+---------+-------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---+\n",
      "|src|dst|\n",
      "+---+---+\n",
      "|  0|  1|\n",
      "|  0|  2|\n",
      "|  0|  3|\n",
      "|  0|  4|\n",
      "|  0|  5|\n",
      "|  1|  0|\n",
      "|  1|  2|\n",
      "|  1|  4|\n",
      "|  1|  5|\n",
      "|  1| 15|\n",
      "|  2|  0|\n",
      "|  2| 11|\n",
      "|  2| 12|\n",
      "|  2| 13|\n",
      "|  2| 14|\n",
      "|  3| 63|\n",
      "|  3| 64|\n",
      "|  3| 65|\n",
      "|  3| 66|\n",
      "|  3| 67|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/19 22:44:48 WARN TaskSetManager: Stage 50 contains a task of very large size (2213 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/10/19 22:44:48 WARN TaskSetManager: Stage 51 contains a task of very large size (1204 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "g.vertices.show()\n",
    "g.edges.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/19 22:44:55 WARN TaskSetManager: Stage 52 contains a task of very large size (2213 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262111, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/19 22:44:56 WARN TaskSetManager: Stage 54 contains a task of very large size (1204 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1234877, 2)\n"
     ]
    }
   ],
   "source": [
    "print((g.vertices.count(), len(g.vertices.columns)))\n",
    "print((g.edges.count(), len(g.edges.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/19 22:45:05 WARN TaskSetManager: Stage 56 contains a task of very large size (1204 KiB). The maximum recommended task size is 1000 KiB.\n",
      "21/10/19 22:45:07 WARN TaskSetManager: Stage 58 contains a task of very large size (1204 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|  id|degree|\n",
      "+----+------+\n",
      "|  26|    17|\n",
      "|  29|    45|\n",
      "| 474|     4|\n",
      "|1677|    13|\n",
      "|1697|     7|\n",
      "| 964|    13|\n",
      "|1806|    18|\n",
      "|1950|     9|\n",
      "|2453|     8|\n",
      "|3091|     8|\n",
      "|2040|     6|\n",
      "|3506|    15|\n",
      "|2214|    24|\n",
      "|2250|     9|\n",
      "|5385|     9|\n",
      "|7225|    13|\n",
      "|2509|     6|\n",
      "|5409|    14|\n",
      "|2529|    18|\n",
      "|2927|     7|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----+--------+\n",
      "|  id|inDegree|\n",
      "+----+--------+\n",
      "|  26|      12|\n",
      "|  29|      40|\n",
      "| 474|       4|\n",
      "|1677|       8|\n",
      "|1697|       2|\n",
      "| 964|       8|\n",
      "|1806|      13|\n",
      "|1950|       4|\n",
      "|2453|       3|\n",
      "|3091|       3|\n",
      "|2040|       1|\n",
      "|3506|      10|\n",
      "|2214|      19|\n",
      "|2250|       4|\n",
      "|5385|       4|\n",
      "|7225|       8|\n",
      "|2509|       1|\n",
      "|5409|       9|\n",
      "|2529|      13|\n",
      "|2927|       2|\n",
      "+----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/19 22:45:07 WARN TaskSetManager: Stage 60 contains a task of very large size (1204 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|  id|outDegree|\n",
      "+----+---------+\n",
      "|  26|        5|\n",
      "|  29|        5|\n",
      "| 964|        5|\n",
      "|1677|        5|\n",
      "|1697|        5|\n",
      "|1806|        5|\n",
      "|1950|        5|\n",
      "|2040|        5|\n",
      "|2214|        5|\n",
      "|2250|        5|\n",
      "|2453|        5|\n",
      "|2509|        5|\n",
      "|2529|        5|\n",
      "|2927|        5|\n",
      "|3091|        5|\n",
      "|3506|        5|\n",
      "|3764|        5|\n",
      "|4590|        5|\n",
      "|4823|        5|\n",
      "|4894|        5|\n",
      "+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.degrees.show()\n",
    "g.inDegrees.show()\n",
    "g.outDegrees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Spark)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
